---
title: "RAG Vision Engine"
emoji: "ğŸ”"
colorFrom: "indigo"
colorTo: "blue"
sdk: "docker"
python_version: "3.12"
app_file: "app.py"
pinned: false
---

# <center>Retrieval-Augmented Generation Vision Engine ğŸ” </center> 

<p align="center">
  <img src="assets/image_1.jpg" width="85%" alt="RAG Vision Engine">
</p>

<p align="center">
  <img src="https://img.shields.io/badge/build-passing-brightgreen" />
  <img src="https://img.shields.io/badge/coverage-80%25-blue" />
  <img src="https://img.shields.io/badge/python-3.12-blue" />
  <img src="https://img.shields.io/badge/docker-ready-2496ED?logo=docker&logoColor=white" />
  <img src="https://img.shields.io/badge/License-MIT-green" />
  <img src="https://img.shields.io/badge/HuggingFace-Space%20Ready-orange?logo=huggingface" />
</p>

---

# ğŸ“˜ What Is RAG Vision Engine?

**RAG Vision Engine** is an experimental **retrieval-augmented computer vision system** combining:

- **CLIP ViT-B/32** â†’ visual patch embedding and retrieval  
- **Qwen2-VL-2B-Instruct** â†’ visionâ€“language reasoning  
- A custom **RAG-style pipeline adapted for images**  
- **FastAPI** backend for inference  
- **Gradio** frontend for interactive usage  

It enables:

- ğŸ·ï¸ **Semi-automatic dataset labeling**  
- ğŸ§  **Visual reasoning over retrieved evidence**  
- ğŸ” **Few-shot classification using support sets**  
- ğŸ–¼ï¸ **Patch-based similarity search**  
- ğŸ“Š **Explainable output with natural language**  

---

## ğŸ“¸ RAG-Vision Summary Table

| Retrieval Evidence | Query Image | System Prompt | User Prompt | Result |
|-------------------|-------------|----------------|-------------|---------|
| <p align="center"><img src="assets/image_2.jpg" width="220"></p> | <p align="center"><img src="assets/image_3.jpg" width="220"></p> | *"You are a visual reasoning engine. Analyze the provided images and determine the condition of the object."* | *"Is the vehicle dirty or clean? Explain your reasoning briefly."* | **Flag:** Dirty<br>**Explanation:** "The vehicle surface appears stained and dusty compared to clean support examples."<br>**Class Scores:** `{dirty: 0.89, clean: 0.11}` |

---

# âœ¨ Key Features

### ğŸ”¹ Retrieval-Augmented Vision (CLIP)
- Extracts patches from support and query images  
- Computes per-patch CLIP embeddings  
- Retrieves the most semantically similar patches  
- Produces per-class similarity scores  

### ğŸ”¹ Vision-Language Reasoning (Qwen2-VL)
- Consumes both the query image and the retrieved patches  
- Uses system + user prompts  
- Outputs:
  - Binary flag  
  - Explanation  
  - Raw VLM transcript  
  - Class similarity map  

### ğŸ”¹ Integrated Full Stack
- **FastAPI** for serving inference  
- **Gradio** for interactive UI  
- Both run inside a **single Docker container**  

---

# ğŸ§  Models Used

## 1. CLIP ViT-B/32  
**Task:** Patch-based embedding + retrieval  
- Converts each image into a grid of embeddings  
- Computes cosine similarity to support patches  
- Provides evidence for Qwen2-VL  

## 2. Qwen2-VL-2B-Instruct  
**Task:** Multimodal reasoning  
- Accepts multiple images  
- Accepts system/user prompts  
- Generates explanations and classifications  

---

## ğŸ” Difference Between Traditional Text RAG and Vision RAG

Traditional **text-based RAG** retrieves relevant text passages (documents, sentences, embeddings) and feeds them to a language model alongside the user query. The model then generates an answer grounded in textual evidence.

**Vision RAG**, on the other hand, retrieves visual evidence. The system extracts image patches from a support dataset using CLIP, embeds them, and retrieves the most similar patches to the query image. These retrieved visual examples â€” together with system and user prompts â€” are then passed to a Vision-Language Model (e.g., Qwen2-VL) to generate a classification flag, explanation, and reasoning.

In short: **text RAG retrieves text, while Vision RAG retrieves images**.  
Both follow the same â€œretrieve â†’ reasonâ€ philosophy, but Vision RAG applies it to the visual domain using multimodal models.

<p align="center">
  <img src="assets/image_4.jpg" width="85%" alt="RAG Vision Engine">
</p>

---

# ğŸ“¦ Project Structure

```text
rag-vision-engine
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ diagram.jpg
â”‚   â”œâ”€â”€ dirty.jpg
â”‚   â””â”€â”€ clean.jpg
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ routers
â”‚   â”‚   â”‚   â””â”€â”€ api_settings.py
â”‚   â”‚   â”œâ”€â”€ routers
â”‚   â”‚   â”‚   â”œâ”€â”€ api_v1
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ healthcheck_ragvision.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag_vision_router.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ support_upload_router.py
â”‚   â”‚   â”‚   â”œâ”€â”€ events
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”‚   â”œâ”€â”€ schema
â”‚   â”‚   â”‚   â”œâ”€â”€ ragvision_inputs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ragvision_outputs.py
â”‚   â”‚   â”‚   â””â”€â”€ support_upload_input.py
â”‚   â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ postprocessor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â”‚   â””â”€â”€ support_index.py
â”‚   â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ base_model_clip.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_model_qwen_vl_2b.py
â”‚   â”‚   â”‚   â”œâ”€â”€ image_utils.py
â”‚   â”‚   â”‚   â””â”€â”€ properties.py
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â”œâ”€â”€ artifacts
â”‚   â”‚   â”œâ”€â”€ clip_vit_base_patch32
â”‚   â”‚   â””â”€â”€ qwen2_vl_2b_instruct
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â””â”€â”€ support
â”‚   â”œâ”€â”€ docs
â”‚   â”‚   â””â”€â”€ openapi.json
â”‚   â”œâ”€â”€ notebooks
â”‚   â”‚   â”œâ”€â”€ clip_local_setup.ipynb
â”‚   â”‚   â”œâ”€â”€ qwen2_vl_2b_instruct_setup.ipynb
â”‚   â”‚   â””â”€â”€ vehicle_dirty_test_with_local_models.ipynb
â”‚   â”œâ”€â”€ test
â”‚   â”‚   â””â”€â”€ pytest
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ assets
â”‚   â”‚   â”œâ”€â”€ examples
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query_image
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ query.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ support
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clean
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clean_1.jpg
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clean_2.jpg
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ clean_2.jpg
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ dirty
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ dirty_1.jpg
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ dirty_2.jpg
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ dirty_3.jpg
â”‚   â”‚   â”‚   â””â”€â”€ example_payload.py
â”‚   â”‚   â””â”€â”€ logos
â”‚   â”‚       â””â”€â”€ logo.jpg
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â””â”€â”€ properties.py
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â””â”€â”€ properties.py
â”‚   â”œâ”€â”€ ui
â”‚   â”‚   â””â”€â”€ layout.py
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ image_utils.py
â”‚   â”‚   â””â”€â”€ text_utils.py
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_all.sh
â”œâ”€â”€ run_api.sh
â””â”€â”€ space.yaml
```

---

# ğŸš€ Running the Project (Docker)

Build the container:

```bash
docker build -t rag-vision .

```
Run
```bash
docker run -p 7860:7860 rag-vision
```

Open:
http://localhost:7860

---

# ğŸ§© Running Locally (Without Docker)

Install dependencies:

```bash
pip install -r requirements.txt
```

```bash
pip install -r requirements.txt
```

Run the unified app:

```bash
python app.py
```

Backend (FastAPI) and frontend (Gradio) start together.

---

# ğŸ”§ API Endpoints

## POST /api/ragvision/invocations

Runs the full RAG-Vision inference pipeline.

## POST /api/support/upload/image

Uploads a single image belonging to a support class.

## GET /api/ragvision/healthcheck

Basic backend healthcheck.

---

# ğŸ“ Example Programmatic Usage
```bash
from frontend.services.rag_api_client import run_rag_inference

result = run_rag_inference(
    system_prompt="Classify the vehicle.",
    user_prompt="Is it dirty or clean?",
    encoded_image=encoded_b64,
    temperature=0.2,
    top_p=0.95,
    k_retrieval=4,
    max_patches_per_class=3,
    max_new_tokens=200,
    input_resolution=224,
    support_res=224,
    support_patch_size=32,
)
```

---

# ğŸ“˜ Configuration Files Explained
Configures:
- Python version
- CPU/GPU resources
- Timeouts
- Container behavior

requirements.txt

Pinned versions of:
- FastAPI
- Gradio
- Torch
- Transformers
- CLIP
- NumPy
- Pillow

---

# ğŸ“„ Full API Specification 
You can find the full OpenAPI schema here:  
â¡ï¸ [`backend/docs/openapi.json`](backend/docs/openapi.json)

---

# ğŸ” License

This project is distributed under the MIT License, allowing reuse and modification.

---

# ğŸ‘¤ Author

Raul Artigues Femenia
- ğŸŒ Website: https://www.raulartigues.com
- ğŸ™ GitHub: https://github.com/raulartigues

---

# â­ Support This Project

If you find this project useful:
- Star â­ the GitHub repo
- Fork and experiment
- Submit issues or PRs

---