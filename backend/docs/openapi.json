{"openapi":"3.1.0","info":{"title":"RAG VISUAL API","description":"\nRAG Vision Engine API provides a Retrieval-Augmented Visual Reasoning system \ndesigned for few-shot image classification, multimodal explanation generation, \nand retrieval-enhanced vision-language inference.\n\nThis API combines two major components:\n\n1. **CLIP ViT-B/32**  \n   Used for patch-level embedding and retrieval across a support image dataset.  \n   The system extracts patches from support images and query images, \n   computes CLIP embeddings, and selects the most similar patches as evidence.\n\n2. **Qwen2-VL-2B-Instruct**  \n   A compact yet powerful vision-language model used for multimodal reasoning.  \n   It receives the query image + retrieved patches along with system and user prompts, \n   and produces:\n   - A binary classification flag  \n   - An explanatory reasoning text  \n   - Class similarity scores  \n   - A full decoded VLM response\n\n---\n\n## üîç Core Features\n\n- Patch-based retrieval using CLIP  \n- Few-shot classification using support images  \n- Multimodal VLM-based reasoning  \n- Support class upload and management  \n- Fully integrated FastAPI backend & Gradio UI  \n- JSON-based remote inference for programmatic usage  \n\n---\n\n## üß© Main Endpoints\n\n### **POST /ragvision/invocations**\nRuns the full RAG-Vision inference pipeline:\n- Decodes the query image\n- Retrieves patch-level evidence using CLIP\n- Sends all context to Qwen2-VL\n- Returns a structured response including flag, explanation, raw response and scores\n\n### **POST /support/upload/image**\nUploads support images for each class (e.g., \"dirty\", \"clean\").\nUsed to build the retrieval index dynamically.\n\n### **GET /ragvision/healthcheck**\nProvides service health status and readiness checks.\n\n---\n\n## üß† Intended Use\n\nThis API is intended for:\n- Semi-automatic dataset labeling  \n- Visual quality inspection  \n- Few-shot visual classification tasks  \n- Vision-language prototyping  \n- Retrieval-augmented AI research  \n\n---\n\n## üì¶ Response Format\n\nMost inference responses follow the `RagVisionOutput` schema:\n- `success`: Boolean status  \n- `flag`: Binary classification output  \n- `explanation`: VLM-generated reasoning  \n- `classScores`: Aggregated similarity scores per class  \n- `rawResponse`: Complete LLM/VLM output  \n- `imageId`: Unique identifier for the inference request  \n\n---\n\n## üìò Notes\n\n- All inference runs are stateless aside from support image uploads.  \n- Support data is stored per-session under `backend/data/support/`.  \n- The system supports CPU or GPU execution depending on environment.  \n- Qwen2-VL and CLIP are loaded locally for optimal performance.\n\n","version":"1.0.0"},"paths":{"/support/upload/image":{"post":{"tags":["RagVision"],"summary":"Upload Single Image","description":"Upload a single support image with automatic class and index handling.\n\nThis endpoint is used to dynamically build or update the support dataset\nused by the RAG-Vision pipeline. Images are uploaded one at a time with\nmetadata describing their class assignment and upload order.\n\nWorkflow:\n    1. Parse and validate index.\n    2. Parse the full list of classes from JSON.\n    3. If index == 0:\n        - Reset (wipe) the entire support folder.\n        - Recreate the support directory structure.\n    4. Validate that the assigned className is part of classes.\n    5. Read and save the uploaded image bytes.\n\nArgs:\n    request (Request):\n        The FastAPI request object (not used directly but required).\n\n    className (str):\n        The name of the class to associate with this image.\n\n    classes (str):\n        A JSON-encoded list of all classes. Required for validation and\n        directory reset operations.\n\n    index (int):\n        Upload index of the image. If index == 0, the support directory\n        is reset and rebuilt from scratch.\n\n    file (UploadFile):\n        The uploaded image file (JPEG/PNG/etc).\n\nReturns:\n    dict:\n        A structured response with:\n            - success (bool)\n            - className (str)\n            - index (int)\n            - filename (str)\n        Or an error message if validation fails.\n\nError Handling:\n    - Invalid index format\n    - Invalid JSON list for 'classes'\n    - Unknown class assignment\n    - Failure to read uploaded file","operationId":"upload_single_image_support_upload_image_post","requestBody":{"content":{"multipart/form-data":{"schema":{"$ref":"#/components/schemas/Body_upload_single_image_support_upload_image_post"}}},"required":true},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"description":"Output schema returned by the RAG Vision Engine.\n\nIncludes structured information about the model execution, classification\nresults, timing data, and potential error messages.","properties":{"success":{"description":"Indicates whether the RAG Vision pipeline completed successfully.","title":"Success","type":"boolean"},"imageId":{"description":"Identifier assigned to the processed image, useful for tracing or logging.","title":"Imageid","type":"string"},"flagKey":{"description":"Identifier associated with the request, used for routing or operational logic.","title":"Flagkey","type":"string"},"flag":{"anyOf":[{"type":"boolean"},{"type":"null"}],"default":null,"description":"Boolean decision or classification result generated by the model, if applicable.","title":"Flag"},"explanation":{"anyOf":[{"type":"string"},{"type":"null"}],"default":null,"description":"Natural-language explanation describing the model's reasoning or justification.","title":"Explanation"},"classScores":{"anyOf":[{"additionalProperties":{"type":"number"},"type":"object"},{"type":"null"}],"default":null,"description":"Dictionary mapping class labels to confidence scores produced during retrieval or classification.","title":"Classscores"},"rawResponse":{"anyOf":[{"type":"string"},{"type":"null"}],"default":null,"description":"Unprocessed or full model response text returned by the generative model.","title":"Rawresponse"},"elapsedTimeMs":{"anyOf":[{"type":"integer"},{"type":"null"}],"default":null,"description":"Total time taken to process the request, measured in milliseconds.","title":"Elapsedtimems"},"error":{"anyOf":[{"type":"string"},{"type":"null"}],"default":null,"description":"Error message in case the request failed, used for debugging and logging.","title":"Error"}},"required":["success","imageId","flagKey"],"title":"RagVisionOutput","type":"object","example":{"success":false,"imageId":"","flagKey":"","elapsedTimeMs":0,"error":""}}}}}}}},"/ragvision/invocations":{"post":{"tags":["RagVision"],"summary":"Rag Vision Inference","description":"Execute a full RAG-Vision pipeline inference request.\n\nThis endpoint orchestrates all components of the RAG-Vision system:\n    1. **Input validation**\n       Ensures all required fields are included in the request payload.\n\n    2. **Support retrieval with CLIP**\n       Uses the SupportIndex to extract and compare visual patches from\n       the support dataset.\n\n    3. **Multimodal reasoning using Qwen2-VL-2B-Instruct**\n       Builds the multimodal input (images + system prompt + user prompt)\n       and performs generative reasoning.\n\n    4. **Postprocessing**\n       Extracts structured fields (flag, explanation) from the raw VLM output.\n\n    5. **Response formatting**\n       Returns a `RagVisionOutput` schema with structured prediction results.\n\nArgs:\n    payload (RagVisionInput):\n        The request payload containing:\n            - Base64-encoded image\n            - System and user prompts\n            - Retrieval configuration\n            - Generation parameters\n            - Image preprocessing settings\n\nReturns:\n    JSONResponse:\n        A JSON-compatible response shaped according to the `RagVisionOutput` model.\n        Contains either:\n            - A successful structured inference result, or\n            - An error structure if execution failed.\n\nNotes:\n    - Each request is assigned a unique `imageId` for tracking.\n    - Errors are logged and returned with full context.\n    - All execution times are measured and logged in milliseconds.","operationId":"rag_vision_inference_ragvision_invocations_post","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/RagVisionInput"}}},"required":true},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/RagVisionOutput"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"description":"Output schema returned by the RAG Vision Engine.\n\nIncludes structured information about the model execution, classification\nresults, timing data, and potential error messages.","properties":{"success":{"description":"Indicates whether the RAG Vision pipeline completed successfully.","title":"Success","type":"boolean"},"imageId":{"description":"Identifier assigned to the processed image, useful for tracing or logging.","title":"Imageid","type":"string"},"flagKey":{"description":"Identifier associated with the request, used for routing or operational logic.","title":"Flagkey","type":"string"},"flag":{"anyOf":[{"type":"boolean"},{"type":"null"}],"default":null,"description":"Boolean decision or classification result generated by the model, if applicable.","title":"Flag"},"explanation":{"anyOf":[{"type":"string"},{"type":"null"}],"default":null,"description":"Natural-language explanation describing the model's reasoning or justification.","title":"Explanation"},"classScores":{"anyOf":[{"additionalProperties":{"type":"number"},"type":"object"},{"type":"null"}],"default":null,"description":"Dictionary mapping class labels to confidence scores produced during retrieval or classification.","title":"Classscores"},"rawResponse":{"anyOf":[{"type":"string"},{"type":"null"}],"default":null,"description":"Unprocessed or full model response text returned by the generative model.","title":"Rawresponse"},"elapsedTimeMs":{"anyOf":[{"type":"integer"},{"type":"null"}],"default":null,"description":"Total time taken to process the request, measured in milliseconds.","title":"Elapsedtimems"},"error":{"anyOf":[{"type":"string"},{"type":"null"}],"default":null,"description":"Error message in case the request failed, used for debugging and logging.","title":"Error"}},"required":["success","imageId","flagKey"],"title":"RagVisionOutput","type":"object","example":{"success":false,"imageId":"","flagKey":"","elapsedTimeMs":0,"error":""}}}}}}}},"/ragvision/healthcheck":{"get":{"tags":["RagVision"],"summary":"Check service health","description":"Healthcheck endpoint for infrastructure monitoring.\n\nThis endpoint verifies the API is alive and responsive.\nUsed by ECS to determine container health.\n\nReturns:\n    dict: A JSON with status \"ok\" and HTTP 201 if successful.","operationId":"healthcheck_ragvision_healthcheck_get","responses":{"201":{"description":"Service is healthy","content":{"application/json":{"schema":{},"example":{"status":"ok"}}}}}}}},"components":{"schemas":{"Body_upload_single_image_support_upload_image_post":{"properties":{"className":{"type":"string","title":"Classname"},"classes":{"type":"string","title":"Classes"},"index":{"type":"integer","title":"Index"},"file":{"type":"string","format":"binary","title":"File"}},"type":"object","required":["className","classes","index","file"],"title":"Body_upload_single_image_support_upload_image_post"},"RagVisionInput":{"properties":{"encodedImage":{"type":"string","title":"Encodedimage","description":"Base64-encoded representation of the input image. May include a data URI prefix."},"flagKey":{"type":"string","title":"Flagkey","description":"Identifier used for routing, feature toggling, or conditional logic within the RAG system."},"systemPrompt":{"type":"string","title":"Systemprompt","description":"System-level instruction that defines behavior and sets constraints before user prompt execution."},"userPrompt":{"type":"string","title":"Userprompt","description":"User instruction or query that the model should respond to using RAG and visual context."},"kRetrieval":{"type":"integer","title":"Kretrieval","description":"Number of relevant patches/items to retrieve during RAG similarity lookup."},"maxPatchesPerClass":{"type":"integer","title":"Maxpatchesperclass","description":"Maximum number of visual patches that can be retrieved per semantic class category."},"maxNewTokens":{"type":"integer","title":"Maxnewtokens","description":"Maximum number of tokens the generative model is allowed to produce in its response."},"inputResolution":{"type":"integer","title":"Inputresolution","description":"Resolution to which the input image will be resized before embedding or patch extraction."},"supportRes":{"type":"integer","title":"Supportres","description":"Resolution applied to support images used during retrieval or CLIP-based similarity matching."},"supportPatchSize":{"type":"integer","title":"Supportpatchsize","description":"Patch size (in pixels) extracted from support images for CLIP similarity scoring."},"temperature":{"type":"number","title":"Temperature","description":"Sampling temperature for generation. Higher values increase randomness."},"topP":{"type":"number","title":"Topp","description":"Nucleus sampling parameter determining probability mass used during generation."},"supportClipLocalDir":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Supportcliplocaldir","description":"Optional local directory path containing the CLIP model used for support retrieval."}},"type":"object","required":["encodedImage","flagKey","systemPrompt","userPrompt","kRetrieval","maxPatchesPerClass","maxNewTokens","inputResolution","supportRes","supportPatchSize","temperature","topP"],"title":"RagVisionInput","description":"Input schema for the RAG Vision Engine.\n\nDefines all parameters required to process an image-based RAG request,\nincluding encoded images, prompts, retrieval settings, and generation\nconfiguration."},"RagVisionOutput":{"properties":{"success":{"type":"boolean","title":"Success","description":"Indicates whether the RAG Vision pipeline completed successfully."},"imageId":{"type":"string","title":"Imageid","description":"Identifier assigned to the processed image, useful for tracing or logging."},"flagKey":{"type":"string","title":"Flagkey","description":"Identifier associated with the request, used for routing or operational logic."},"flag":{"anyOf":[{"type":"boolean"},{"type":"null"}],"title":"Flag","description":"Boolean decision or classification result generated by the model, if applicable."},"explanation":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Explanation","description":"Natural-language explanation describing the model's reasoning or justification."},"classScores":{"anyOf":[{"additionalProperties":{"type":"number"},"type":"object"},{"type":"null"}],"title":"Classscores","description":"Dictionary mapping class labels to confidence scores produced during retrieval or classification."},"rawResponse":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Rawresponse","description":"Unprocessed or full model response text returned by the generative model."},"elapsedTimeMs":{"anyOf":[{"type":"integer"},{"type":"null"}],"title":"Elapsedtimems","description":"Total time taken to process the request, measured in milliseconds."},"error":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Error","description":"Error message in case the request failed, used for debugging and logging."}},"type":"object","required":["success","imageId","flagKey"],"title":"RagVisionOutput","description":"Output schema returned by the RAG Vision Engine.\n\nIncludes structured information about the model execution, classification\nresults, timing data, and potential error messages."}}}}