{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2632c6",
   "metadata": {},
   "source": [
    "<h1><center>CLIP Local Model Setup & Preparation</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7ee47",
   "metadata": {},
   "source": [
    "This script downloads the CLIP ViT-B/32 model and processor from Hugging Face and saves them locally into the project’s artifacts/clip_vit_base_patch32 directory.\n",
    "It ensures the folder exists, loads the pretrained weights, and stores both the model and processor so they can be used offline in future inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590b9f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CLIP model to: ../artifacts/clip_vit_base_patch32\n",
      "CLIP model successfully downloaded and stored locally.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import os\n",
    "\n",
    "LOCAL_MODEL_DIR = os.path.join(\"..\", \"artifacts\", \"clip_vit_base_patch32\")\n",
    "\n",
    "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "print(\"Downloading CLIP model to:\", LOCAL_MODEL_DIR)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "clip_model.save_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "clip_processor.save_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "print(\"CLIP model successfully downloaded and stored locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc44519",
   "metadata": {},
   "source": [
    "This script loads a locally stored CLIP ViT-B/32 model and processor from the artifacts/clip_vit_base_patch32 directory.\n",
    "It automatically selects GPU (cuda) if available, otherwise defaults to CPU.\n",
    "The model is initialized in evaluation mode and ready for downstream inference tasks.\n",
    "The script also prints the dimensionality of the visual features produced by CLIP’s vision encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853be7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model from local directory: ../artifacts/clip_vit_base_patch32\n",
      "CLIP model successfully loaded from local storage.\n",
      "Vision feature dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "LOCAL_MODEL_DIR = os.path.join(\"..\", \"artifacts\", \"clip_vit_base_patch32\")\n",
    "\n",
    "print(\"Loading CLIP model from local directory:\", LOCAL_MODEL_DIR)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(LOCAL_MODEL_DIR).to(DEVICE).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(LOCAL_MODEL_DIR)\n",
    "\n",
    "print(\"CLIP model successfully loaded from local storage.\")\n",
    "print(\"Vision feature dimension:\", clip_model.vision_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b9b58",
   "metadata": {},
   "source": [
    "This script loads a sample image from the project’s assets/ folder and processes it using the locally loaded CLIP ViT-B/32 model.\n",
    "The image is converted to RGB, passed through the CLIP vision encoder, and the script prints the shape of the extracted patch-level embeddings (excluding the CLS token).\n",
    "This helps verify that the model and processor are working correctly and that embeddings can be generated for downstream tasks such as visual retrieval or RAG-vision pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c96bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test image from: ../../assets/sample_dirty.jpg\n",
      "Embeddings shape: torch.Size([1, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "TEST_IMAGE = os.path.join(\"..\", \"..\", \"assets\", \"sample_dirty.jpg\")\n",
    "\n",
    "print(\"Loading test image from:\", TEST_IMAGE)\n",
    "\n",
    "img = Image.open(TEST_IMAGE).convert(\"RGB\")\n",
    "\n",
    "inputs = clip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = clip_model.vision_model(pixel_values=inputs[\"pixel_values\"])\n",
    "    emb = out.last_hidden_state[:, 1:, :]\n",
    "    print(\"Embeddings shape:\", emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9eb3f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-vision-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
