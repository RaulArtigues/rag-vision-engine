{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886762c6",
   "metadata": {},
   "source": [
    "<h1><center>Vehicle Dirt Classification Test Using Local CLIP + Qwen2-VL-2B-Instruct</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a0b2b",
   "metadata": {},
   "source": [
    "This code defines the system and user prompts used to guide the vision-language model during inference. The system prompt establishes strict rules for determining visible dirt on vehicles, ensuring consistent, expert-level reasoning. The user prompt provides the model with the query image and retrieved visual evidence, instructing it to return only a clean, structured output containing a boolean dirt flag and a short explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05738d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in visual dirt assessment for vehicles.\n",
    "Rules:\n",
    "- Mark VisibleDirtyFlag = True ONLY if there is real dirt (dust, mud, stains).\n",
    "- Ignore reflections, shadows, car paint color, camera artifacts.\n",
    "- If dirt is minimal or ambiguous, output VisibleDirtyFlag = False.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Here is:\n",
    "1. The new vehicle image.\n",
    "2. Visually retrieved support examples (dirty and clean patches).\n",
    "\n",
    "Use these examples to reason.\n",
    "\n",
    "RETURN ONLY:\n",
    "VisibleDirtyFlag: True/False\n",
    "Explanation: <short text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5b887",
   "metadata": {},
   "source": [
    "This script implements a complete vision-based Retrieval-Augmented Generation (RAG) pipeline using locally stored models. It loads CLIP to extract patch-level embeddings from both support images and the query image, building an in-memory vector store that separates “dirty” and “clean” vehicle examples. The system performs visual retrieval by comparing query patches against all stored patches, aggregating similarity scores and selecting representative evidence images for downstream reasoning.\n",
    "\n",
    "Using these retrieved visual examples, the script then applies the Qwen2-VL-2B-Instruct model—also loaded fully offline—to generate a structured assessment of whether the vehicle appears dirty. The query image and the retrieved evidence patches are embedded into a multimodal chat template, enabling the model to reason about dirt presence. The final output includes raw VLM predictions and an automatically parsed VisibleDirtyFlag value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a30577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag-vision-engine/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP from local directory: ../artifacts/clip_vit_base_patch32\n",
      "CLIP Vision Dimension: 768\n",
      "\n",
      "Loading support images...\n",
      "\n",
      "Added support image: ../data/support/dirty/side-view-very-dirty-car-600nw-2296491121.jpg.jpg (dirty) → 49 patches\n",
      "Added support image: ../data/support/dirty/79086697-off-road-vehicle-after-driving-in-the-rain-on-extremely-dirty-rural-road.jpg (dirty) → 49 patches\n",
      "Added support image: ../data/support/dirty/16474284186573.jpg (dirty) → 49 patches\n",
      "Added support image: ../data/support/clean/mg-zs-hybrid-front-view.jpg (clean) → 49 patches\n",
      "Added support image: ../data/support/clean/1140-subaru-forester-sport-hero-esp.jpg (clean) → 49 patches\n",
      "Added support image: ../data/support/clean/GAC-Eco-Amigable.jpg (clean) → 49 patches\n",
      "\n",
      "Vector store ready!\n",
      "Total vectors: 294\n",
      "Vector dimension: 768\n",
      "\n",
      "Loading VLM: Qwen2-VL-2B-Instruct from: ../artifacts/qwen2_vl_2b_instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag-vision-engine/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 29.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query image: ../../assets/sample_dirty.jpg\n",
      "\n",
      "Scores: {'dirty': 94.14763367176056, 'clean': 48.16642606258392}\n",
      "\n",
      "=== RAW MODEL RESPONSE ===\n",
      "system\n",
      "You are an expert in visual dirt assessment for vehicles.\n",
      "Rules:\n",
      "- Mark VisibleDirtyFlag = True ONLY if there is real dirt (dust, mud, stains).\n",
      "- Ignore reflections, shadows, car paint color, camera artifacts.\n",
      "- If dirt is minimal or ambiguous, output VisibleDirtyFlag = False.\n",
      "user\n",
      "Here is:\n",
      "1. The new vehicle image.\n",
      "2. Visually retrieved support examples (dirty and clean patches).\n",
      "\n",
      "Use these examples to reason.\n",
      "\n",
      "RETURN ONLY:\n",
      "VisibleDirtyFlag: True/False\n",
      "Explanation: <short text>\n",
      "assistant\n",
      "VisibleDirtyFlag: True\n",
      "Explanation: The vehicle is covered in mud, indicating real dirt.\n",
      "\n",
      "Parsed Flag: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    ")\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CLIP_LOCAL_DIR = os.path.join(\"..\", \"artifacts\", \"clip_vit_base_patch32\")\n",
    "QWEN_LOCAL_DIR = os.path.join(\"..\", \"artifacts\", \"qwen2_vl_2b_instruct\")\n",
    "\n",
    "SUPPORT_ROOT = os.path.join(\"..\", \"data\", \"support\")\n",
    "DIRTY_DIR = os.path.join(SUPPORT_ROOT, \"dirty\")\n",
    "CLEAN_DIR = os.path.join(SUPPORT_ROOT, \"clean\")\n",
    "\n",
    "QUERY_IMAGE = os.path.join(\"..\", \"..\", \"assets\", \"sample_dirty.jpg\")\n",
    "\n",
    "PATCH_SIZE = 32\n",
    "RES = 224\n",
    "VALID_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "print(\"Loading CLIP from local directory:\", CLIP_LOCAL_DIR)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_LOCAL_DIR).to(DEVICE).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_LOCAL_DIR)\n",
    "\n",
    "VISION_DIM = clip_model.vision_model.config.hidden_size\n",
    "print(\"CLIP Vision Dimension:\", VISION_DIM)\n",
    "\n",
    "def get_patch_embeddings(img):\n",
    "    inputs = clip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = clip_model.vision_model(pixel_values=inputs[\"pixel_values\"])\n",
    "        tokens = out.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    tokens = tokens.squeeze(0)\n",
    "    tokens = tokens / tokens.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    n = tokens.shape[0]\n",
    "    s = int(math.sqrt(n))\n",
    "    return tokens.reshape(s, s, VISION_DIM)\n",
    "\n",
    "def patch_coords(image_size=(RES, RES), patch_size=PATCH_SIZE):\n",
    "    W, H = image_size\n",
    "    coords = []\n",
    "    for j in range(H // patch_size):\n",
    "        for i in range(W // patch_size):\n",
    "            x0, y0 = i * patch_size, j * patch_size\n",
    "            coords.append((x0, y0, x0 + patch_size, y0 + patch_size))\n",
    "    return coords\n",
    "\n",
    "support_vectors = None\n",
    "meta = []\n",
    "\n",
    "def is_image_file(path):\n",
    "    return os.path.splitext(path.lower())[1] in VALID_EXTS\n",
    "\n",
    "def add_support_image_to_memory(path, label):\n",
    "    global support_vectors, meta\n",
    "\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(\"Skipping invalid image:\", path, \"->\", e)\n",
    "        return\n",
    "\n",
    "    img_r = img.resize((RES, RES), Image.BICUBIC)\n",
    "    grid = get_patch_embeddings(img_r).detach().cpu().numpy()\n",
    "    vecs = grid.reshape(-1, VISION_DIM).astype(np.float32)\n",
    "\n",
    "    support_vectors = vecs if support_vectors is None else np.vstack([support_vectors, vecs])\n",
    "\n",
    "    boxes = patch_coords()\n",
    "\n",
    "    for p_idx, box in enumerate(boxes):\n",
    "        meta.append({\n",
    "            \"label\": label,\n",
    "            \"image_path\": path,\n",
    "            \"patch_idx\": p_idx,\n",
    "            \"box\": box\n",
    "        })\n",
    "\n",
    "    print(f\"Added support image: {path} ({label}) → {vecs.shape[0]} patches\")\n",
    "\n",
    "print(\"\\nLoading support images...\\n\")\n",
    "\n",
    "for fname in os.listdir(DIRTY_DIR):\n",
    "    p = os.path.join(DIRTY_DIR, fname)\n",
    "    if is_image_file(p):\n",
    "        add_support_image_to_memory(p, \"dirty\")\n",
    "\n",
    "for fname in os.listdir(CLEAN_DIR):\n",
    "    p = os.path.join(CLEAN_DIR, fname)\n",
    "    if is_image_file(p):\n",
    "        add_support_image_to_memory(p, \"clean\")\n",
    "\n",
    "print(\"\\nVector store ready!\")\n",
    "print(\"Total vectors:\", support_vectors.shape[0])\n",
    "print(\"Vector dimension:\", support_vectors.shape[1])\n",
    "\n",
    "def retrieve_for_query_image(image_path, k=3):\n",
    "    img = Image.open(image_path).convert(\"RGB\").resize((RES, RES))\n",
    "\n",
    "    grid = get_patch_embeddings(img).detach().cpu().numpy()\n",
    "    vecs = grid.reshape(-1, VISION_DIM).astype(np.float32)\n",
    "\n",
    "    S = support_vectors.astype(np.float32)\n",
    "\n",
    "    class_score = {\"dirty\": 0.0, \"clean\": 0.0}\n",
    "    evidence = []\n",
    "\n",
    "    for q_idx in range(vecs.shape[0]):\n",
    "        sims = S @ vecs[q_idx]\n",
    "        top = sims.argsort()[-k:][::-1]\n",
    "\n",
    "        for idx in top:\n",
    "            sim = float(sims[idx])\n",
    "            ref = meta[idx]\n",
    "            class_score[ref[\"label\"]] += sim\n",
    "            evidence.append({\"sim\": sim, \"ref\": ref})\n",
    "\n",
    "    evidence.sort(key=lambda e: e[\"sim\"], reverse=True)\n",
    "    return class_score, evidence, img\n",
    "\n",
    "def crop_patch(image_path, box):\n",
    "    img = Image.open(image_path).convert(\"RGB\").resize((RES, RES))\n",
    "    return img.crop(box)\n",
    "\n",
    "print(\"\\nLoading VLM: Qwen2-VL-2B-Instruct from:\", QWEN_LOCAL_DIR)\n",
    "\n",
    "vlm_processor = AutoProcessor.from_pretrained(QWEN_LOCAL_DIR)\n",
    "vlm_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    QWEN_LOCAL_DIR,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    ")\n",
    "vlm_model.to(DEVICE).eval()\n",
    "\n",
    "print(f\"\\nProcessing query image: {QUERY_IMAGE}\")\n",
    "\n",
    "class_score, evidence, q_img = retrieve_for_query_image(QUERY_IMAGE, k=4)\n",
    "\n",
    "print(\"\\nScores:\", class_score)\n",
    "\n",
    "dirty_patches = [e for e in evidence if e[\"ref\"][\"label\"] == \"dirty\"][:3]\n",
    "clean_patches = [e for e in evidence if e[\"ref\"][\"label\"] == \"clean\"][:3]\n",
    "\n",
    "rag_images = [q_img] + [\n",
    "    crop_patch(e[\"ref\"][\"image_path\"], e[\"ref\"][\"box\"])\n",
    "    for e in (dirty_patches + clean_patches)\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT.strip()},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            *[{\"type\": \"image\"} for _ in rag_images],\n",
    "            {\"type\": \"text\", \"text\": USER_PROMPT.strip()}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = vlm_processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = vlm_processor(\n",
    "    text=prompt,\n",
    "    images=rag_images,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs = {k: (v.to(DEVICE) if isinstance(v, torch.Tensor) else v) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = vlm_model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "resp = vlm_processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n=== RAW MODEL RESPONSE ===\")\n",
    "print(resp)\n",
    "\n",
    "match = re.findall(r\"VisibleDirtyFlag:\\s*(True|False)\", resp)\n",
    "\n",
    "if match:\n",
    "    print(\"\\nParsed Flag:\", match[-1])\n",
    "else:\n",
    "    print(\"\\nCould not parse VisibleDirtyFlag.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bfff2",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-vision-engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
